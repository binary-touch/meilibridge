# MeiliBridge Configuration Example
# Copy this file to config.yaml and update with your settings

app:
  name: "MeiliBridge"
  instance_id: "dev-01"

# Option 1: Single source configuration (backward compatible)
# source:
#   type: postgresql
#   host: "localhost"
#   port: 5432
#   database: "myapp"
#   user: "postgres"
#   password: "${POSTGRES_PASSWORD}"
#   pool:
#     max_size: 10
#     min_idle: 2
#   slot_name: "meilibridge_slot"
#   publication: "meilibridge_pub"
#   statement_cache:
#     enabled: true
#     max_size: 100

# Option 2: Multiple sources configuration (recommended for multi-database)
sources:
  - name: "primary"
    type: postgresql
    host: "localhost"
    port: 5432
    database: "myapp"
    user: "postgres"
    password: "${POSTGRES_PASSWORD}"
    pool:
      max_size: 10
      min_idle: 2
    slot_name: "meilibridge_slot_primary"
    publication: "meilibridge_pub_primary"
    statement_cache:
      enabled: true
      max_size: 100
      
  - name: "secondary"
    type: postgresql
    host: "secondary.example.com"
    port: 5432
    database: "analytics"
    user: "postgres"
    password: "${SECONDARY_DB_PASSWORD}"
    pool:
      max_size: 5
      min_idle: 1
    slot_name: "meilibridge_slot_secondary"
    publication: "meilibridge_pub_secondary"
    statement_cache:
      enabled: true
      max_size: 50

meilisearch:
  url: "http://localhost:7700"
  api_key: "${MEILI_MASTER_KEY}"
  timeout: 30
  primary_key: "id"  # Default primary key for all indexes
  auto_create_index: true  # Auto-create missing indexes

redis:
  url: "redis://localhost:6379"
  key_prefix: "meilibridge"

sync_tasks:
  # Sync from primary source
  - id: "users_sync"
    source_name: "primary"        # Specify which source to use
    table: "public.users"
    index: "users"
    primary_key: "id"
    full_sync_on_start: true
    
    options:
      batch_size: 1000
      batch_timeout_ms: 1000
  
  # Sync from secondary source
  - id: "analytics_events_sync"
    source_name: "secondary"      # Different source
    table: "public.analytics_events"
    index: "analytics_events"
    primary_key: "event_id"
    full_sync_on_start: false     # CDC only
    
    options:
      batch_size: 500
      batch_timeout_ms: 2000
      
    # Soft delete configuration (optional)
    soft_delete:
      field: "status"             # Field to check for soft delete
      delete_values:              # Values that indicate deletion
        - "DELETED"
        - "INACTIVE"
        - false                   # Can also check boolean fields
      handle_on_full_sync: true   # Filter out during full sync
      handle_on_cdc: true         # Transform UPDATE to DELETE during CDC

api:
  enabled: true
  host: "0.0.0.0"
  port: 7701

logging:
  level: "info"
  format: "pretty"

features:
  auto_recovery: true
  health_checks: true
  metrics_export: true
  distributed_mode: false

# Performance configuration
performance:
  # Parallel processing for high throughput
  parallel_processing:
    enabled: false                   # Enable for high-volume deployments
    workers_per_table: 4            # Number of parallel workers per table
    max_concurrent_events: 1000     # Max events processed concurrently
    work_stealing: true             # Enable load balancing between tables
    work_steal_interval_ms: 100     # How often to check for work stealing
    work_steal_threshold: 50        # Min queue size difference for stealing
  
  # Batch processing settings
  batch_processing:
    default_batch_size: 100
    max_batch_size: 1000
    min_batch_size: 10
    batch_timeout_ms: 5000
    adaptive_batching: true         # Enable dynamic batch sizing
    
    # Adaptive batching configuration
    adaptive_config:
      target_latency_ms: 1000       # Target processing time per batch
      adjustment_factor: 0.2        # How aggressively to adjust (0.0-1.0)
      metric_window_size: 10        # Number of metrics to average
      adjustment_interval_ms: 5000  # Min time between adjustments
      memory_pressure_threshold: 80.0  # Memory usage % to reduce batch size
      per_table_optimization: true  # Optimize batch size per table
  
  # Connection pool settings
  connection_pool:
    max_connections: 20
    min_connections: 5
    connection_timeout: 30
    idle_timeout: 600

# Error handling configuration  
error_handling:
  retry:
    enabled: true
    max_attempts: 3
    initial_backoff_ms: 100
    max_backoff_ms: 30000
    backoff_multiplier: 2.0
    jitter_factor: 0.1
  
  dead_letter_queue:
    enabled: true
    storage: "memory"               # Options: memory, redis
    max_entries_per_task: 10000
    retention_hours: 24

# Monitoring configuration
monitoring:
  metrics_enabled: true
  metrics_interval_seconds: 60
  health_checks_enabled: true
  health_check_interval_seconds: 30

# Exactly-once delivery configuration
exactly_once_delivery:
  enabled: true                       # Enable exactly-once delivery guarantees
  deduplication_window: 10000         # Number of events to track for deduplication
  transaction_timeout_secs: 30        # Timeout for two-phase commit transactions
  two_phase_commit: true              # Use two-phase commit protocol
  checkpoint_before_write: true       # Save checkpoint atomically before Meilisearch write